{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "BASE = '../Datasets/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122265, 7) (25080, 3) (25080, 2)\n"
     ]
    }
   ],
   "source": [
    "census = pd.read_csv(BASE + 'census_starter.csv')\n",
    "train = pd.read_csv(BASE + 'train.csv')\n",
    "test = pd.read_csv(BASE + 'test.csv')\n",
    "sub = pd.read_csv(BASE + 'sample_submission.csv')\n",
    "print(train.shape, test.shape, sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test\n",
    "train['istest'] = 0\n",
    "test['istest'] = 1\n",
    "raw = pd.concat((train, test)).sort_values(['cfips', 'row_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['first_day_of_month'] = pd.to_datetime(raw[\"first_day_of_month\"])\n",
    "raw['county'] = raw.groupby('cfips')['county'].ffill()\n",
    "raw['state'] = raw.groupby('cfips')['state'].ffill()\n",
    "raw[\"year\"] = raw[\"first_day_of_month\"].dt.year\n",
    "raw[\"month\"] = raw[\"first_day_of_month\"].dt.month\n",
    "raw[\"dcount\"] = raw.groupby(['cfips'])['row_id'].cumcount() #タイムステップの作成\n",
    "raw['county_i'] = (raw['county'] + raw['state']).factorize()[0] #カテゴリデータを数値変換\n",
    "raw['state_i'] = raw['state'].factorize()[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pct_bb_2017\n",
      "pct_bb_2018\n",
      "pct_bb_2019\n",
      "pct_bb_2020\n",
      "pct_bb_2021\n",
      "pct_college_2017\n",
      "pct_college_2018\n",
      "pct_college_2019\n",
      "pct_college_2020\n",
      "pct_college_2021\n",
      "pct_foreign_born_2017\n",
      "pct_foreign_born_2018\n",
      "pct_foreign_born_2019\n",
      "pct_foreign_born_2020\n",
      "pct_foreign_born_2021\n",
      "pct_it_workers_2017\n",
      "pct_it_workers_2018\n",
      "pct_it_workers_2019\n",
      "pct_it_workers_2020\n",
      "pct_it_workers_2021\n",
      "median_hh_inc_2017\n",
      "median_hh_inc_2018\n",
      "median_hh_inc_2019\n",
      "median_hh_inc_2020\n",
      "median_hh_inc_2021\n"
     ]
    }
   ],
   "source": [
    "categories = ['pct_bb', 'pct_college','pct_foreign_born', 'pct_it_workers','median_hh_inc']\n",
    "years = list(raw['year'].unique())\n",
    "\n",
    "for category in categories:\n",
    "\traw[category] = np.nan\n",
    "\n",
    "\tfor year in years:\n",
    "\t\tcensus_year = year - 2\n",
    "\t\tcensus_column = category + f'_{census_year}'\n",
    "\t\tprint(census_column)\n",
    "\n",
    "\t\tfor id in raw['cfips'].unique():\n",
    "\t\t\traw.loc[(raw['cfips'] == id) & (raw['year']==year), [category]] = census[census_column][census['cfips']==id].iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are some anomalies, specially at timestep 18¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad2161f22bc469fa350da88a1bc2dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosuke-konno\\AppData\\Local\\Temp\\ipykernel_26044\\2908064762.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  var[:i] *= (var[i]/var[i-1])\n",
      "C:\\Users\\kosuke-konno\\AppData\\Local\\Temp\\ipykernel_26044\\2908064762.py:15: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  var[:i] *= (var[i]/var[i-1])\n",
      "C:\\Users\\kosuke-konno\\AppData\\Local\\Temp\\ipykernel_26044\\2908064762.py:15: RuntimeWarning: invalid value encountered in multiply\n",
      "  var[:i] *= (var[i]/var[i-1])\n",
      "C:\\Users\\kosuke-konno\\AppData\\Local\\Temp\\ipykernel_26044\\2908064762.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  difa = abs(var[i] - var[i-1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(481, 732)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = []\n",
    "cnt = 0\n",
    "\n",
    "for o in tqdm(raw.cfips.unique()):\n",
    "\tindices = (raw['cfips'] == o) #各郡の行を抜き出す\n",
    "\ttmp = raw.loc[indices].copy().reset_index(drop=True)\n",
    "\tvar = tmp.microbusiness_density.values.copy()\n",
    "\n",
    "\tfor i in range(37, 2, -1):\n",
    "\t\t#当該期間までの平均の20％を閾値とする\n",
    "\t\tthr = 0.2*np.mean(var[:i])\n",
    "\t\tdifa = abs(var[i] - var[i-1])\n",
    "\t\tif (difa >= thr):\n",
    "\t\t\t#閾値を超えている場合はその比率を前の値に掛ける\n",
    "\t\t\tvar[:i] *= (var[i]/var[i-1])\n",
    "\t\t\toutliers.append(o)\n",
    "\t\t\tcnt+=1\n",
    "\t\n",
    "\tvar[0] = var[1] * 0.99\n",
    "\traw.loc[indices, 'microbusiness_density'] = var\n",
    "\n",
    "outliers = np.unique(outliers)\n",
    "len(outliers), cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#1ヶ月前のMBDを入れる。bfillによって、直後のRowの数字が入る。\n",
    "lag = 1\n",
    "raw[f'mbd_lag_{lag}'] = raw.groupby('cfips')['microbusiness_density'].shift(lag).bfill()\n",
    "#1ヶ月前からの変化量を作る。NAであれば0。clipによって下限が0となっている。\n",
    "raw['dif'] = (raw['microbusiness_density'] / raw[f'mbd_lag_{lag}']).fillna(1).clip(0, None) - 1\n",
    "#1ヶ月前のMBDが0であれば変化量も0とする。\n",
    "raw.loc[(raw[f'mbd_lag_{lag}']==0), 'dif'] = 0\n",
    "#1ヶ月前のMBDが0かつ当期が0以上であれば変化量を1とする。\n",
    "raw.loc[(raw[f'microbusiness_density']>0) & (raw[f'mbd_lag_{lag}']==0), 'dif'] = 1\n",
    "raw['dif'] = raw['dif'].abs()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMAPE is a relative metric so target must be converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['target'] = raw.groupby('cfips')['microbusiness_density'].shift(-1)\n",
    "raw['target'] = raw['target']/raw['microbusiness_density'] - 1\n",
    "\n",
    "#MBが0の時の欠損値を0にしておく\n",
    "raw.loc[raw['cfips']==28055, 'target'] = 0.0\n",
    "raw.loc[raw['cfips']==48269, 'target'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最後にActiveだったMicrobusinessの数\n",
    "raw['lastactive'] = raw.groupby('cfips')['active'].transform('last')\n",
    "\n",
    "#学習データ1-28、Val29-38、Test、39-48\n",
    "dt = raw.loc[raw.dcount==28].groupby('cfips')['microbusiness_density'].agg('last')\n",
    "raw['lasttarget'] = raw['cfips'].map(dt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(raw, target='microbusiness_density', target_act='active_tmp', lags = 6):\n",
    "    feats = []\n",
    "    for lag in range(1, lags):\n",
    "        raw[f'mbd_lag_{lag}'] = raw.groupby('cfips')[target].shift(lag)\n",
    "        raw[f'act_lag_{lag}'] = raw.groupby('cfips')[target_act].diff(lag)\n",
    "        feats.append(f'mbd_lag_{lag}')\n",
    "        feats.append(f'act_lag_{lag}')\n",
    "        \n",
    "    lag = 1\n",
    "    for window in [2, 4, 6]:\n",
    "        raw[f'mbd_rollmea{window}_{lag}'] = raw.groupby('cfips')[f'mbd_lag_{lag}'].transform(lambda s: s.rolling(window, min_periods=1).sum())        \n",
    "        #raw[f'mbd_rollmea{window}_{lag}'] = raw[f'mbd_lag_{lag}'] - raw[f'mbd_rollmea{window}_{lag}']\n",
    "        feats.append(f'mbd_rollmea{window}_{lag}')\n",
    "        \n",
    "    return raw, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['mbd_lag_1', 'act_lag_1', 'mbd_lag_2', 'act_lag_2', 'mbd_lag_3', 'act_lag_3', 'mbd_rollmea2_1', 'mbd_rollmea4_1', 'mbd_rollmea6_1', 'pct_bb', 'pct_college', 'median_hh_inc']\n"
     ]
    }
   ],
   "source": [
    "# Build Features based in lag of target\n",
    "raw, feats = build_features(raw, 'target', 'active', lags = 4)\n",
    "features = ['state_i']\n",
    "features += feats\n",
    "features +=categories\n",
    "\n",
    "to_remove = ['state_i','pct_foreign_born', 'pct_it_workers']\n",
    "\n",
    "for item in to_remove:\n",
    "\tfeatures.remove(item)\n",
    "\n",
    "print(len(features))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Epoch 1/200\n",
      "631/631 [==============================] - 8s 7ms/step - loss: 0.1594 - mae: 0.1594 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 2/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0209 - mae: 0.0209 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 3/200\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 4/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 5/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 6/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 7/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 8/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 9/200\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 10/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 11/200\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 12/200\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 13/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 14/200\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 15/200\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 16/200\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 17/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 18/200\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 19/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 20/200\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 21/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 22/200\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 23/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 24/200\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 25/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 26/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 27/200\n",
      "631/631 [==============================] - 4s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 28/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 29/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 30/200\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 31/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 32/200\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 33/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 34/200\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 35/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 36/200\n",
      "631/631 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 37/200\n",
      "631/631 [==============================] - 6s 9ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 38/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 39/200\n",
      "631/631 [==============================] - 6s 10ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 40/200\n",
      "631/631 [==============================] - 6s 10ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 41/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 42/200\n",
      "631/631 [==============================] - 6s 10ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 43/200\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "98/98 [==============================] - 0s 2ms/step\n",
      "TS: 29\n",
      "Last Value SMAPE: 1.0868726017655663\n",
      "XGB SMAPE: 1.0765291553174003\n",
      "\n",
      "30\n",
      "Epoch 1/200\n",
      "654/654 [==============================] - 11s 13ms/step - loss: 0.1907 - mae: 0.1907 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 2/200\n",
      "654/654 [==============================] - 7s 11ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 3/200\n",
      "654/654 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 4/200\n",
      "654/654 [==============================] - 5s 7ms/step - loss: 0.0040 - mae: 0.0040 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 5/200\n",
      "654/654 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 6/200\n",
      "654/654 [==============================] - 4s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 7/200\n",
      "654/654 [==============================] - 6s 9ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 8/200\n",
      "654/654 [==============================] - 6s 9ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 9/200\n",
      "654/654 [==============================] - 6s 9ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 10/200\n",
      "654/654 [==============================] - 6s 8ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 11/200\n",
      "654/654 [==============================] - 5s 8ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 12/200\n",
      "654/654 [==============================] - 5s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 13/200\n",
      "654/654 [==============================] - 5s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 14/200\n",
      "654/654 [==============================] - 6s 9ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 15/200\n",
      "654/654 [==============================] - 6s 9ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 16/200\n",
      "654/654 [==============================] - 6s 9ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 17/200\n",
      "654/654 [==============================] - 5s 8ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 18/200\n",
      "654/654 [==============================] - 5s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 19/200\n",
      "654/654 [==============================] - 4s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 20/200\n",
      "654/654 [==============================] - 4s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 21/200\n",
      "654/654 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 22/200\n",
      "654/654 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 23/200\n",
      "654/654 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 24/200\n",
      "654/654 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 25/200\n",
      "654/654 [==============================] - 4s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 26/200\n",
      "654/654 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 27/200\n",
      "654/654 [==============================] - 6s 9ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 28/200\n",
      "654/654 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 29/200\n",
      "654/654 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 30/200\n",
      "654/654 [==============================] - 4s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 31/200\n",
      "654/654 [==============================] - 4s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 32/200\n",
      "654/654 [==============================] - 4s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "98/98 [==============================] - 0s 2ms/step\n",
      "TS: 30\n",
      "Last Value SMAPE: 1.318087470449913\n",
      "XGB SMAPE: 1.283650529408535\n",
      "\n",
      "31\n",
      "Epoch 1/200\n",
      "676/676 [==============================] - 8s 9ms/step - loss: 0.1996 - mae: 0.1996 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 2/200\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 3/200\n",
      "676/676 [==============================] - 6s 9ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 4/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0040 - mae: 0.0040 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 5/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 6/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 7/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 8/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 9/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 10/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 11/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 12/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 13/200\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 14/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 15/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 16/200\n",
      "676/676 [==============================] - 4s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 17/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 18/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 19/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 20/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 21/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 22/200\n",
      "676/676 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 23/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 24/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 25/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 26/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 27/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 28/200\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 29/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 30/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 31/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 32/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 33/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 34/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 35/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 36/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 37/200\n",
      "676/676 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 38/200\n",
      "676/676 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 39/200\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 40/200\n",
      "676/676 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "98/98 [==============================] - 0s 2ms/step\n",
      "TS: 31\n",
      "Last Value SMAPE: 1.1258309832479911\n",
      "XGB SMAPE: 1.108891210148314\n",
      "\n",
      "32\n",
      "Epoch 1/200\n",
      "699/699 [==============================] - 6s 6ms/step - loss: 0.1817 - mae: 0.1817 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 2/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0431 - mae: 0.0431 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 3/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0116 - mae: 0.0116 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 4/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 5/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 6/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 7/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 8/200\n",
      "699/699 [==============================] - 5s 6ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 9/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 10/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 11/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 12/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 13/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 14/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 15/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 16/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 17/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 18/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 19/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 20/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 21/200\n",
      "699/699 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 22/200\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 23/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 24/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 25/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 26/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 27/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 28/200\n",
      "699/699 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 29/200\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "98/98 [==============================] - 0s 1ms/step\n",
      "TS: 32\n",
      "Last Value SMAPE: 0.897969439640235\n",
      "XGB SMAPE: 0.8914241035077434\n",
      "\n",
      "33\n",
      "Epoch 1/200\n",
      "721/721 [==============================] - 5s 5ms/step - loss: 0.1160 - mae: 0.1160 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 2/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0208 - mae: 0.0208 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 3/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 4/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 5/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 6/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 7/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 8/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 9/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 10/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 11/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 12/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 13/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 14/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 15/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 16/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 17/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 18/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 19/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 20/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 21/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 22/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 23/200\n",
      "721/721 [==============================] - 4s 5ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 24/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 25/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 26/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 27/200\n",
      "721/721 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 28/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 29/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 30/200\n",
      "721/721 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 31/200\n",
      "721/721 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 32/200\n",
      "721/721 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 33/200\n",
      "721/721 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 34/200\n",
      "721/721 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 35/200\n",
      "721/721 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 36/200\n",
      "721/721 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 37/200\n",
      "721/721 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 38/200\n",
      "721/721 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "98/98 [==============================] - 0s 2ms/step\n",
      "TS: 33\n",
      "Last Value SMAPE: 1.3686285670946152\n",
      "XGB SMAPE: 1.340831149522042\n",
      "\n",
      "34\n",
      "Epoch 1/200\n",
      "744/744 [==============================] - 9s 8ms/step - loss: 0.1595 - mae: 0.1595 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 2/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 3/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 4/200\n",
      "744/744 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 5/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 6/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 7/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 8/200\n",
      "744/744 [==============================] - 7s 9ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 9/200\n",
      "744/744 [==============================] - 7s 9ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 10/200\n",
      "744/744 [==============================] - 8s 10ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 11/200\n",
      "744/744 [==============================] - 8s 11ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 12/200\n",
      "744/744 [==============================] - 7s 9ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 13/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 14/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 15/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 16/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 17/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 18/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 19/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 20/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 21/200\n",
      "744/744 [==============================] - 6s 9ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 22/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 23/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 24/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 25/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0213 - val_mae: 0.0213\n",
      "Epoch 26/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 27/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 28/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 29/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 30/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 31/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 32/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 33/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 34/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 35/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 36/200\n",
      "744/744 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 37/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 38/200\n",
      "744/744 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 39/200\n",
      "744/744 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 40/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 41/200\n",
      "744/744 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 42/200\n",
      "744/744 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 43/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 44/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 45/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 46/200\n",
      "744/744 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "98/98 [==============================] - 0s 2ms/step\n",
      "TS: 34\n",
      "Last Value SMAPE: 2.2033066808448543\n",
      "XGB SMAPE: 2.120960234235638\n",
      "\n",
      "35\n",
      "Epoch 1/200\n",
      "767/767 [==============================] - 7s 7ms/step - loss: 0.1921 - mae: 0.1921 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 2/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0197 - mae: 0.0197 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 3/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 4/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 5/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 6/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 7/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 8/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 9/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 10/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 11/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 12/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 13/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 14/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 15/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 16/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 17/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 18/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 19/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 20/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 21/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 22/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 23/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 24/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 25/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 26/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 27/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 28/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 29/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 30/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 31/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 32/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 33/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 34/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 35/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 36/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 37/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 38/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 39/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 40/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 41/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 42/200\n",
      "767/767 [==============================] - 7s 9ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 43/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 44/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 45/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 46/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 47/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 48/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 49/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 50/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 51/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 52/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 53/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 54/200\n",
      "767/767 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 55/200\n",
      "767/767 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 56/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 57/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 58/200\n",
      "767/767 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "98/98 [==============================] - 0s 2ms/step\n",
      "TS: 35\n",
      "Last Value SMAPE: 1.2797936949214384\n",
      "XGB SMAPE: 1.2669635862209934\n",
      "\n",
      "36\n",
      "Epoch 1/200\n",
      "789/789 [==============================] - 8s 7ms/step - loss: 0.1676 - mae: 0.1676 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 2/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0250 - mae: 0.0250 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 3/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0042 - mae: 0.0042 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 4/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 5/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 6/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0040 - mae: 0.0040 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 7/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 8/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 9/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 10/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 11/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 12/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0108 - val_mae: 0.0108\n",
      "Epoch 13/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0108 - val_mae: 0.0108\n",
      "Epoch 14/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 15/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 16/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 17/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 18/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 19/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 20/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 21/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 22/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 23/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 24/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 25/200\n",
      "789/789 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 26/200\n",
      "789/789 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 27/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 28/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 29/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 30/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 31/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 32/200\n",
      "789/789 [==============================] - 7s 9ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 33/200\n",
      "789/789 [==============================] - 7s 9ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 34/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 35/200\n",
      "789/789 [==============================] - 7s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 36/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 37/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 38/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 39/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 40/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 41/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 42/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 43/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 44/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 45/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 46/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 47/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 48/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 49/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 50/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 51/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 52/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 53/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 54/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 55/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 56/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 57/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 58/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 59/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 60/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 61/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 62/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 63/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 64/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 65/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 66/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 67/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 68/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 69/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 70/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 71/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 72/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 73/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 74/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 75/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 76/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 77/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 78/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 79/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 80/200\n",
      "789/789 [==============================] - 7s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 81/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 82/200\n",
      "789/789 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 83/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 84/200\n",
      "789/789 [==============================] - 6s 8ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 85/200\n",
      "789/789 [==============================] - 5s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "98/98 [==============================] - 0s 2ms/step\n",
      "TS: 36\n",
      "Last Value SMAPE: 1.034314865882525\n",
      "XGB SMAPE: 1.0036034721602887\n",
      "\n",
      "37\n",
      "Epoch 1/200\n",
      "812/812 [==============================] - 8s 7ms/step - loss: 0.1572 - mae: 0.1572 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 2/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 3/200\n",
      "812/812 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 4/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 5/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 6/200\n",
      "812/812 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 7/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 8/200\n",
      "812/812 [==============================] - 5s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 9/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0039 - mae: 0.0039 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 10/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 11/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 12/200\n",
      "812/812 [==============================] - 5s 7ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 13/200\n",
      "812/812 [==============================] - 5s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 14/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 15/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 16/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 17/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 18/200\n",
      "812/812 [==============================] - 5s 6ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 19/200\n",
      "812/812 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 20/200\n",
      "812/812 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 21/200\n",
      "812/812 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 22/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 23/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 24/200\n",
      "812/812 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 25/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 26/200\n",
      "812/812 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 27/200\n",
      "812/812 [==============================] - 6s 8ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 28/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 29/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 30/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 31/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 32/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 33/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 34/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 35/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 36/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0035 - mae: 0.0035 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 37/200\n",
      "812/812 [==============================] - 6s 7ms/step - loss: 0.0034 - mae: 0.0034 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "98/98 [==============================] - 0s 2ms/step\n",
      "TS: 37\n",
      "Last Value SMAPE: 1.101119095956366\n",
      "XGB SMAPE: 1.076411950922852\n",
      "\n",
      "XGB SMAPE: 1.2410294879382007\n",
      "Last Value SMAPE: 1.2684359333115005\n"
     ]
    }
   ],
   "source": [
    "from Scripts.metrics import smape, vsmape\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "blacklist = [\n",
    "    'North Dakota', 'Iowa', 'Kansas', 'Nebraska', 'South Dakota','New Mexico', 'Alaska', 'Vermont'\n",
    "]\n",
    "\n",
    "blacklistcfips = [\n",
    "1019,1027,1029,1035,1039,1045,1049,1057,1067,1071,1077,1085,1091,1099,1101,1123,1131,1133,4001,4012,4013,4021,4023,5001,5003,5005,5017,5019,5027,5031,5035,5047,5063,5065,5071,5081,5083,5087,5091,5093,5107,5109,5115,5121,5137,5139,5141,5147,6003,6015,6027,6033,6053,6055,6057,6071,6093,6097,6103,6105,6115,8003,8007,8009,8019,8021,8023,8047,8051,8053,8055,8057,8059,8061,8065,8067,8069,8071,8073,8075,8085,8091,8093,8097,8099,8103,8105,8107,8109,8111,8115,8117,8121,9007,9009,9015,12009,12017,12019,12029,12047,12055,12065,12075,12093,12107,12127,13005,13007,13015,13017,13019,13027,13035,13047,13065,13081,13083,13099,13107,13109,13117,13119,13121,13123,13125,13127,13135,13143,13147,13161,13165,13171,13175,13181,13193,13201,13221,13225,13229,13231,13233,13245,13247,13249,13257,13279,13281,13287,13289,13293,13301,13319,15001,15005,15007,16001,16003,16005,16007,16013,16015,16017,16023,16025,16029,16031,16033,16035,16037,16043,16045,16049,16061,16063,16067,17001,17003,17007,17009,17013,17015,17023,17025,17031,17035,17045,17051,17059,17061,17063,17065,17067,17069,17075,17077,17081,17085,17087,17103,17105,17107,17109,17115,17117,17123,17127,17133,17137,17141,17143,17147,17153,17167,17169,17171,17177,17179,17181,17185,17187,17193,18001,18007,18009,18013,18015,18019,18021,18025,18035,18037,18039,18041,18053,18061,18075,18079,18083,18087,18099,18103,18111,18113,18115,18137,18139,18145,18153,18171,18179,21001,21003,21013,21017,21023,21029,21035,21037,21039,21045,21047,21055,21059,21065,21075,21077,21085,21091,21093,21097,21099,21101,21103,21115,21125,21137,21139,21141,21149,21155,21157,21161,21165,21179,21183,21191,21197,21199,21215,21217,21223,21227,21237,21239,22019,22021,22031,22039,22041,22047,22069,22085,22089,22101,22103,22109,22111,22115,22119,22121,23003,23009,23021,23027,23029,24011,24027,24029,24031,24035,24037,24039,24041,25011,25015,26003,26007,26011,26019,26021,26025,26027,26033,26037,26041,26043,26051,26053,26057,26059,26061,26065,26071,26077,26079,26083,26089,26097,26101,26103,26109,26111,26115,26117,26119,26127,26129,26131,26135,26141,26143,26155,26161,26165,27005,27011,27013,27015,27017,27021,27023,27025,27029,27047,27051,27055,27057,27065,27069,27073,27075,27077,27079,27087,27091,27095,27101,27103,27105,27107,27109,27113,27117,27119,27123,27125,27129,27131,27133,27135,27141,27147,27149,27155,27159,27167,27169,28017,28019,28023,28025,28035,28045,28049,28061,28063,28093,28097,28099,28125,28137,28139,28147,28159,29001,29015,29019,29031,29033,29041,29049,29051,29055,29057,29063,29065,29069,29075,29085,29089,29101,29103,29111,29121,29123,29125,29135,29137,29139,29143,29157,29159,29161,29167,29171,29173,29175,29177,29183,29195,29197,29199,29203,29205,29207,29209,29213,29215,29217,29223,29227,29229,30005,30009,30025,30027,30033,30035,30037,30039,30045,30049,30051,30053,30055,30057,30059,30069,30071,30073,30077,30079,30083,30085,30089,30091,30093,30101,30103,30105,30107,30109,32005,32009,32017,32023,32027,32029,32510,33005,33007,34021,34027,34033,34035,36011,36017,36023,36033,36043,36047,36049,36051,36057,36061,36067,36083,36091,36097,36103,36107,36113,36115,36121,36123,37005,37009,37011,37017,37023,37029,37031,37049,37061,37075,37095,37117,37123,37131,37137,37151,37187,37189,37197,39005,39009,39015,39017,39019,39023,39037,39039,39043,39049,39053,39057,39063,39067,39071,39077,39085,39087,39091,39097,39105,39107,39113,39117,39119,39125,39127,39129,39135,39137,39151,39153,39157,40003,40013,40015,40023,40025,40027,40035,40039,40043,40045,40053,40055,40057,40059,40065,40067,40073,40077,40079,40099,40105,40107,40111,40115,40123,40127,40129,40133,40141,40147,40151,40153,41001,41007,41013,41015,41017,41021,41025,41031,41033,41037,41051,41055,41063,41067,41069,42005,42007,42011,42013,42015,42019,42027,42029,42031,42035,42053,42057,42067,42071,42083,42085,42093,42097,42105,42111,42113,42115,42123,42125,42127,42129,44005,44007,44009,45001,45009,45021,45025,45031,45059,45067,45071,45073,45089,47001,47005,47013,47015,47019,47021,47023,47027,47035,47039,47041,47047,47055,47057,47059,47061,47069,47073,47075,47077,47083,47087,47099,47105,47121,47127,47131,47133,47135,47137,47147,47151,47153,47159,47161,47163,47169,47177,47183,47185,48001,48011,48017,48019,48045,48057,48059,48063,48065,48073,48077,48079,48081,48083,48087,48095,48101,48103,48107,48109,48115,48117,48119,48123,48125,48129,48149,48151,48153,48155,48159,48161,48165,48175,48189,48191,48195,48197,48211,48221,48229,48233,48235,48237,48239,48241,48243,48245,48255,48261,48263,48265,48267,48269,48275,48277,48283,48293,48299,48305,48311,48313,48319,48321,48323,48327,48333,48345,48347,48355,48369,48377,48379,48383,48387,48389,48401,48403,48413,48417,48431,48433,48437,48443,48447,48453,48455,48457,48461,48463,48465,48469,48471,48481,48483,48485,48487,48495,48499,49001,49009,49013,49019,49027,49031,49045,51005,51017,51025,51029,51031,51036,51037,51043,51057,51059,51065,51071,51073,51077,51079,51083,51091,51095,51097,51101,51111,51115,51119,51121,51127,51135,51147,51155,51159,51165,51167,51171,51173,51181,51183,51191,51197,51530,51590,51610,51620,51670,51678,51720,51735,51750,51770,51810,51820,53013,53019,53023,53031,53033,53037,53039,53041,53047,53065,53069,53071,53075,54013,54019,54025,54031,54033,54041,54049,54055,54057,54063,54067,54071,54077,54079,54085,54089,54103,55001,55003,55005,55007,55011,55017,55021,55025,55029,55037,55043,55047,55049,55051,55061,55065,55067,55075,55077,55091,55097,55101,55103,55109,55117,55123,55125,55127,56007,56009,56011,56015,56017,56019,56021,56027,56031,56037,56043,56045,\n",
    "12061,  6095, 49025, 18073, 29029, 29097, 48419, 51830, 30067, 26095, 18159, 32001, 54065, 54027, 13043, 48177, 55069, 48137, 30087, 29007, 13055, 48295, 28157, 29037, 45061, 22053, 13199, 47171, 53001, 55041, 51195, 18127, 29151, 48307, 51009, 16047, 29133,  5145, 17175, 21027, 48357, 29179, 13023, 16077, 48371, 21057, 16039, 21143, 48435, 48317, 48475,  5129, 36041, 48075, 29017, 47175, 39167, 47109, 17189, 17173, 28009, 39027, 48133, 18129, 48217, 40081, 36021,  6005, 42099, 18051, 36055, 53051, 6109, 21073, 27019,  6051, 48055,  8083, 48503, 17021, 10003, 41061, 22001, 22011, 21205, 48223, 51103, 51047, 16069, 17033, 41011,  6035, 47145, 27083, 18165, 36055, 12001, 26159,  8125, 34017,\n",
    "28141, 55119, 48405, 40029, 18125, 21135, 29073, 55115, 37149,55039, 26029, 12099, 13251, 48421, 39007, 41043, 22015, 37115,54099, 51137, 22049, 55131, 17159, 56001, 40005, 18017, 28091,47101, 27037, 29005, 13239, 21019, 55085, 48253, 51139, 40101,13283, 18049, 39163, 45049, 51113,\n",
    "]\n",
    "\n",
    "ACT_THR = 1.8\n",
    "ABS_THR = 1.00\n",
    "raw['ypred_last'] = np.nan\n",
    "raw['ypred'] = np.nan\n",
    "raw['k'] = 1.\n",
    "VAL = []\n",
    "BEST_ROUNDS = []\n",
    "\n",
    "for TS in range(29, 38):\n",
    "    print(TS)\n",
    "\n",
    "    #学習データと検証データ\n",
    "    train_indices = (raw.istest==0) & (raw.dcount  < TS) & (raw.dcount >= 1) & (raw.lastactive>ACT_THR)  & (raw.lasttarget>ABS_THR) \n",
    "    valid_indices = (raw.istest==0) & (raw.dcount == TS)\n",
    "\n",
    "    X_train = raw.loc[train_indices, features]\n",
    "    y_train = raw.loc[train_indices, 'target'].clip(-0.0043, 0.0045)\n",
    "\n",
    "    X_val = raw.loc[valid_indices, features]\n",
    "    y_val =  raw.loc[valid_indices, 'target']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = np.nan_to_num(scaler.fit_transform(X_train), nan=0)\n",
    "    X_val = np.nan_to_num(scaler.transform(X_val), nan=0)\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal', input_shape=X_train.shape[1:]),\n",
    "        keras.layers.Dropout(0.2), \n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(96, activation='relu', kernel_initializer='he_normal'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam', metrics='mae')\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        patience=20, \n",
    "        restore_best_weights=True\n",
    "        )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs = 200,\n",
    "        batch_size = 128,\n",
    "        verbose = 1,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks = [early_stopping]\n",
    "    )\n",
    "\n",
    "    # best_rounds = model.best_iteration\n",
    "    # BEST_ROUNDS.append(model.best_iteration)\n",
    "    ypred = model.predict(X_val)\n",
    "    raw.loc[valid_indices, 'k'] = ypred + 1\n",
    "    raw.loc[valid_indices,'k'] = raw.loc[valid_indices,'k'] * raw.loc[valid_indices,'microbusiness_density']\n",
    "\n",
    "    # Validate\n",
    "    lastval = raw.loc[raw.dcount==TS, ['cfips', 'microbusiness_density']].set_index('cfips').to_dict()['microbusiness_density']\n",
    "    dt = raw.loc[raw.dcount==TS, ['cfips', 'k']].set_index('cfips').to_dict()['k']\n",
    "    \n",
    "    df = raw.loc[raw.dcount==(TS+1), ['cfips', 'microbusiness_density', 'state', 'lastactive', 'mbd_lag_1']].reset_index(drop=True)\n",
    "    df['pred'] = df['cfips'].map(dt)\n",
    "    df['lastval'] = df['cfips'].map(lastval)\n",
    "    \n",
    "    df.loc[df['lastactive']<=ACT_THR, 'pred'] = df.loc[df['lastactive']<=ACT_THR, 'lastval']\n",
    "    df.loc[df['lastval']<=ABS_THR, 'pred'] = df.loc[df['lastval']<=ABS_THR, 'lastval']\n",
    "    df.loc[df['state'].isin(blacklist), 'pred'] = df.loc[df['state'].isin(blacklist), 'lastval']\n",
    "    df.loc[df['cfips'].isin(blacklistcfips), 'pred'] = df.loc[df['cfips'].isin(blacklistcfips), 'lastval']\n",
    "    raw.loc[raw.dcount==(TS+1), 'ypred'] = df['pred'].values\n",
    "    raw.loc[raw.dcount==(TS+1), 'ypred_last'] = df['lastval'].values\n",
    "\n",
    "    print(f'TS: {TS}')\n",
    "    print('Last Value SMAPE:', smape(df['microbusiness_density'], df['lastval']) )\n",
    "    print('XGB SMAPE:', smape(df['microbusiness_density'], df['pred']))\n",
    "    print()\n",
    "\n",
    "\n",
    "ind = (raw.dcount>=30)&(raw.dcount<=38)\n",
    "print( 'XGB SMAPE:', smape( raw.loc[ind, 'microbusiness_density'],  raw.loc[ind, 'ypred'] ) )\n",
    "print( 'Last Value SMAPE:', smape( raw.loc[ind, 'microbusiness_density'],  raw.loc[ind, 'ypred_last'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>microbusiness_density</th>\n",
       "      <th>ypred</th>\n",
       "      <th>error</th>\n",
       "      <th>error_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.334431</td>\n",
       "      <td>3.301528</td>\n",
       "      <td>0.991672</td>\n",
       "      <td>1.135557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>7.823300</td>\n",
       "      <td>7.739399</td>\n",
       "      <td>1.078225</td>\n",
       "      <td>1.155810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.206827</td>\n",
       "      <td>1.187995</td>\n",
       "      <td>1.572734</td>\n",
       "      <td>1.687769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.236650</td>\n",
       "      <td>1.215874</td>\n",
       "      <td>1.694227</td>\n",
       "      <td>1.834867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1.777708</td>\n",
       "      <td>1.754580</td>\n",
       "      <td>1.309475</td>\n",
       "      <td>1.403959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147140</th>\n",
       "      <td>2.892446</td>\n",
       "      <td>2.926768</td>\n",
       "      <td>1.179620</td>\n",
       "      <td>1.179620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147187</th>\n",
       "      <td>25.438322</td>\n",
       "      <td>25.550702</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.368550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147234</th>\n",
       "      <td>3.954258</td>\n",
       "      <td>3.758343</td>\n",
       "      <td>5.080377</td>\n",
       "      <td>5.183206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147281</th>\n",
       "      <td>3.027295</td>\n",
       "      <td>3.027295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147328</th>\n",
       "      <td>1.749688</td>\n",
       "      <td>1.749688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3135 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        microbusiness_density      ypred     error  error_last\n",
       "30                   3.334431   3.301528  0.991672    1.135557\n",
       "77                   7.823300   7.739399  1.078225    1.155810\n",
       "124                  1.206827   1.187995  1.572734    1.687769\n",
       "171                  1.236650   1.215874  1.694227    1.834867\n",
       "218                  1.777708   1.754580  1.309475    1.403959\n",
       "...                       ...        ...       ...         ...\n",
       "147140               2.892446   2.926768  1.179620    1.179620\n",
       "147187              25.438322  25.550702  0.440800    0.368550\n",
       "147234               3.954258   3.758343  5.080377    5.183206\n",
       "147281               3.027295   3.027295  0.000000    0.000000\n",
       "147328               1.749688   1.749688  0.000000    0.000000\n",
       "\n",
       "[3135 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['error'] = vsmape(raw['microbusiness_density'], raw['ypred'])\n",
    "raw['error_last'] = vsmape(raw['microbusiness_density'], raw['ypred_last'])\n",
    "raw.loc[(raw.dcount==30), ['microbusiness_density', 'ypred', 'error', 'error_last'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosuke-konno\\AppData\\Local\\Temp\\ipykernel_27956\\956906285.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  dt = raw.loc[(raw.dcount>=30)&(raw.dcount<=38) ].groupby(['cfips','dcount'])['error', 'error_last'].last()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = raw.loc[(raw.dcount>=30)&(raw.dcount<=38) ].groupby(['cfips','dcount'])['error', 'error_last'].last()\n",
    "dt['miss'] = dt['error'] > dt['error_last']\n",
    "dt = dt.groupby('cfips')['miss'].mean()\n",
    "dt = dt.loc[dt>=0.50]\n",
    "dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dt.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(861.3333333333334, 810.0, [810, 570, 842, 1525, 608, 441, 1417, 718, 821])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean( BEST_ROUNDS ), np.median( BEST_ROUNDS ), BEST_ROUNDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rounds = int(np.median( BEST_ROUNDS )+1)\n",
    "best_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kosuke-konno\\Miniconda3\\envs\\ai\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "c:\\Users\\kosuke-konno\\Miniconda3\\envs\\ai\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    }
   ],
   "source": [
    "TS = 38\n",
    "print(TS)\n",
    "\n",
    "model0 = xgb.XGBRegressor(\n",
    "    objective='reg:pseudohubererror',\n",
    "    #objective='reg:squarederror',\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=best_rounds,\n",
    "    learning_rate=0.0075,\n",
    "    max_leaves = 31,\n",
    "    subsample=0.60,\n",
    "    colsample_bytree=0.50,\n",
    "    max_bin=4096,\n",
    "    n_jobs=2,\n",
    "    eval_metric='mae',\n",
    ")\n",
    "model1 = xgb.XGBRegressor(\n",
    "    objective='reg:pseudohubererror',\n",
    "    #objective='reg:squarederror',\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=best_rounds,\n",
    "    learning_rate=0.0075,\n",
    "    max_leaves = 31,\n",
    "    subsample=0.60,\n",
    "    colsample_bytree=0.50,\n",
    "    max_bin=4096,\n",
    "    n_jobs=2,\n",
    "    eval_metric='mae',\n",
    ")\n",
    "\n",
    "train_indices = (raw.istest==0) & (raw.dcount  < TS) & (raw.dcount >= 1) & (raw.lastactive>ACT_THR)  & (raw.lasttarget>ABS_THR) \n",
    "valid_indices = (raw.dcount == TS)\n",
    "model0.fit(\n",
    "    raw.loc[train_indices, features],\n",
    "    raw.loc[train_indices, 'target'].clip(-0.0044, 0.0046),\n",
    ")\n",
    "model1.fit(\n",
    "    raw.loc[train_indices, features],\n",
    "    raw.loc[train_indices, 'target'].clip(-0.0044, 0.0046),\n",
    ")\n",
    "\n",
    "ypred = (model0.predict(raw.loc[valid_indices, features]) + model1.predict(raw.loc[valid_indices, features]))/2\n",
    "raw.loc[valid_indices, 'k'] = ypred + 1.\n",
    "raw.loc[valid_indices,'k'] = raw.loc[valid_indices,'k'] * raw.loc[valid_indices,'microbusiness_density']\n",
    "\n",
    "# Validate\n",
    "lastval = raw.loc[raw.dcount==TS, ['cfips', 'microbusiness_density']].set_index('cfips').to_dict()['microbusiness_density']\n",
    "dt = raw.loc[raw.dcount==TS, ['cfips', 'k']].set_index('cfips').to_dict()['k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw.loc[raw.dcount==(TS+1), ['cfips', 'microbusiness_density', 'state', 'lastactive', 'mbd_lag_1']].reset_index(drop=True)\n",
    "df['pred'] = df['cfips'].map(dt)\n",
    "df['lastval'] = df['cfips'].map(lastval)\n",
    "\n",
    "df.loc[df['lastactive']<=ACT_THR, 'pred'] = df.loc[df['lastactive']<=ACT_THR, 'lastval']\n",
    "df.loc[df['lastval']<=ABS_THR, 'pred'] = df.loc[df['lastval']<=ABS_THR, 'lastval']\n",
    "df.loc[df['state'].isin(blacklist), 'pred'] = df.loc[df['state'].isin(blacklist), 'lastval']\n",
    "df.loc[df['cfips'].isin(blacklistcfips), 'pred'] = df.loc[df['cfips'].isin(blacklistcfips), 'lastval']\n",
    "raw.loc[raw.dcount==(TS+1), 'ypred'] = df['pred'].values\n",
    "raw.loc[raw.dcount==(TS+1), 'ypred_last'] = df['lastval'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>cfips</th>\n",
       "      <th>microbusiness_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1001_2022-11-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>3.468046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1001_2022-12-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>3.468046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1001_2023-01-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>3.468046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1001_2023-02-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>3.468046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1001_2023-03-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>3.468046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1001_2023-04-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>3.468046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1001_2023-05-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>3.468046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1001_2023-06-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>3.468046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1003_2022-11-01</td>\n",
       "      <td>1003</td>\n",
       "      <td>8.378113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1003_2022-12-01</td>\n",
       "      <td>1003</td>\n",
       "      <td>8.378113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1003_2023-01-01</td>\n",
       "      <td>1003</td>\n",
       "      <td>8.378113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1003_2023-02-01</td>\n",
       "      <td>1003</td>\n",
       "      <td>8.378113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1003_2023-03-01</td>\n",
       "      <td>1003</td>\n",
       "      <td>8.378113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1003_2023-04-01</td>\n",
       "      <td>1003</td>\n",
       "      <td>8.378113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1003_2023-05-01</td>\n",
       "      <td>1003</td>\n",
       "      <td>8.378113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1003_2023-06-01</td>\n",
       "      <td>1003</td>\n",
       "      <td>8.378113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1005_2022-11-01</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1005_2022-12-01</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1005_2023-01-01</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1005_2023-02-01</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1005_2023-03-01</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1005_2023-04-01</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1005_2023-05-01</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1005_2023-06-01</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1007_2022-11-01</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.288621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1007_2022-12-01</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.288621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1007_2023-01-01</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.288621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1007_2023-02-01</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.288621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1007_2023-03-01</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.288621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1007_2023-04-01</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.288621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1007_2023-05-01</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.288621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1007_2023-06-01</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.288621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>1009_2022-11-01</td>\n",
       "      <td>1009</td>\n",
       "      <td>1.833883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1009_2022-12-01</td>\n",
       "      <td>1009</td>\n",
       "      <td>1.833883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>1009_2023-01-01</td>\n",
       "      <td>1009</td>\n",
       "      <td>1.833883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1009_2023-02-01</td>\n",
       "      <td>1009</td>\n",
       "      <td>1.833883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>1009_2023-03-01</td>\n",
       "      <td>1009</td>\n",
       "      <td>1.833883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1009_2023-04-01</td>\n",
       "      <td>1009</td>\n",
       "      <td>1.833883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>1009_2023-05-01</td>\n",
       "      <td>1009</td>\n",
       "      <td>1.833883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>1009_2023-06-01</td>\n",
       "      <td>1009</td>\n",
       "      <td>1.833883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              row_id  cfips  microbusiness_density\n",
       "39   1001_2022-11-01   1001               3.468046\n",
       "40   1001_2022-12-01   1001               3.468046\n",
       "41   1001_2023-01-01   1001               3.468046\n",
       "42   1001_2023-02-01   1001               3.468046\n",
       "43   1001_2023-03-01   1001               3.468046\n",
       "44   1001_2023-04-01   1001               3.468046\n",
       "45   1001_2023-05-01   1001               3.468046\n",
       "46   1001_2023-06-01   1001               3.468046\n",
       "86   1003_2022-11-01   1003               8.378113\n",
       "87   1003_2022-12-01   1003               8.378113\n",
       "88   1003_2023-01-01   1003               8.378113\n",
       "89   1003_2023-02-01   1003               8.378113\n",
       "90   1003_2023-03-01   1003               8.378113\n",
       "91   1003_2023-04-01   1003               8.378113\n",
       "92   1003_2023-05-01   1003               8.378113\n",
       "93   1003_2023-06-01   1003               8.378113\n",
       "133  1005_2022-11-01   1005               1.233400\n",
       "134  1005_2022-12-01   1005               1.233400\n",
       "135  1005_2023-01-01   1005               1.233400\n",
       "136  1005_2023-02-01   1005               1.233400\n",
       "137  1005_2023-03-01   1005               1.233400\n",
       "138  1005_2023-04-01   1005               1.233400\n",
       "139  1005_2023-05-01   1005               1.233400\n",
       "140  1005_2023-06-01   1005               1.233400\n",
       "180  1007_2022-11-01   1007               1.288621\n",
       "181  1007_2022-12-01   1007               1.288621\n",
       "182  1007_2023-01-01   1007               1.288621\n",
       "183  1007_2023-02-01   1007               1.288621\n",
       "184  1007_2023-03-01   1007               1.288621\n",
       "185  1007_2023-04-01   1007               1.288621\n",
       "186  1007_2023-05-01   1007               1.288621\n",
       "187  1007_2023-06-01   1007               1.288621\n",
       "227  1009_2022-11-01   1009               1.833883\n",
       "228  1009_2022-12-01   1009               1.833883\n",
       "229  1009_2023-01-01   1009               1.833883\n",
       "230  1009_2023-02-01   1009               1.833883\n",
       "231  1009_2023-03-01   1009               1.833883\n",
       "232  1009_2023-04-01   1009               1.833883\n",
       "233  1009_2023-05-01   1009               1.833883\n",
       "234  1009_2023-06-01   1009               1.833883"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "time = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "raw.loc[raw['cfips']==28055, 'microbusiness_density'] = 0\n",
    "raw.loc[raw['cfips']==48269, 'microbusiness_density'] = 1.762115\n",
    "\n",
    "dt = raw.loc[raw.dcount==39, ['cfips', 'ypred']].set_index('cfips').to_dict()['ypred']\n",
    "test = raw.loc[raw.istest==1, ['row_id', 'cfips','microbusiness_density']].copy()\n",
    "test['microbusiness_density'] = test['cfips'].map(dt)\n",
    "\n",
    "test[['row_id','microbusiness_density']].to_csv('../Submission/submission_XGBE_'+time+'.csv', index=False)\n",
    "test.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "time = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "a = pd.read_csv('submission1.csv')\n",
    "b = pd.read_csv('submission2.csv')\n",
    "\n",
    "a['microbusiness_density'] *= 0.15\n",
    "a['microbusiness_density'] += b['microbusiness_density'] * 0.85\n",
    "\n",
    "a.to_csv('../Submission/submission_mix_'+time+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45b2f238c9039db59b6338dbfafc2d51b84d6ab03ac1e3d0f0d70f6bf0e756a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
